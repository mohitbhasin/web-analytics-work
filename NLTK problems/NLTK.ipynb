{
 "metadata": {
  "name": "",
  "signature": "sha256:b59f62fad7447254df8b5844bcaab7630c1b6a0b3c94fbf41ab14ff9a9e5599b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Q\n",
      "import re\n",
      "def split_sentence(line):\n",
      "    \"\"\"Split a sentence into individual words by splitting at punctuations and whitespace\"\"\"\n",
      "    data = re.findall(r\"[\\w']+\", line)\n",
      "    return data\n",
      "\n",
      "def main():\n",
      "    x = \"this! is,a , sentence...\\n\\n\"  # take care to remove any null strings at the end\n",
      "    #x = \"a.b,c,.d,ef,gh \"\n",
      "    ans = split_sentence(x)\n",
      "    print ans\n",
      "\n",
      "if __name__ == \"__main__\": \n",
      "    import sys \n",
      "    if len(sys.argv) == 2 and sys.argv[1] == \"--test\":  # valid if we run as python prog.py --test\n",
      "        run_tests()\n",
      "    else:\n",
      "        main()\n",
      "    \n",
      "x = \"this! is,a , sentence...\\n\\n\"\n",
      "assert split_sentence(x) == ['this', 'is', 'a', 'sentence']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['this', 'is', 'a', 'sentence']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "def split_sentence(line):\n",
      "    \"\"\"Split a sentence into individual words by splitting at punctuations and whitespace\"\"\"\n",
      "    tokenizer = RegexpTokenizer(r'\\w+')\n",
      "    data = tokenizer.tokenize(line)\n",
      "    print data\n",
      "        \n",
      "def main():\n",
      "    x = \"this! is,a , sentence...\\n\\n\"  # take care to remove any null strings at the end\n",
      "    split_sentence(x)\n",
      "\n",
      "if __name__ == \"__main__\": \n",
      "    import sys \n",
      "    if len(sys.argv) == 2 and sys.argv[1] == \"--test\":  # valid if we run as python prog.py --test\n",
      "        run_tests()\n",
      "    else:\n",
      "        main()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['this', 'is', 'a', 'sentence']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_counts(fname):\n",
      "    \"\"\"Give the counts of different parts-of-speech occurrences in the given document\"\"\"\n",
      "    # Your code here to print the counts\n",
      "    \n",
      "# If file 'x.txt' contains the following sentence:\n",
      "# The quick brown fox jumps over the lazy dog\n",
      "# You should get something like ('NN': 5, 'DT': 2, 'NNS': 1, 'IN': 1)\n",
      "# Notice that the NLTK tagger can potentially tag jumps as Noun!\n",
      "\n",
      "\n",
      "\n",
      "import nltk.data\n",
      "from nltk.util import ngrams\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "noun =0\n",
      "dt = 0\n",
      "vb = 0\n",
      "pre = 0\n",
      "adj = 0\n",
      "\n",
      "text=open('article.txt').read()\n",
      "\n",
      "def pos_counts(text):\n",
      "    sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "    sentences = sentence_splitter.tokenize(text)\n",
      "    print 'NUMBER OF SENTENCES: '+ str(len(sentences))\n",
      "    for sentence in sentences:\n",
      "        noun =0\n",
      "        dt = 0\n",
      "        vb = 0\n",
      "        pre = 0\n",
      "        adj = 0\n",
      "        token = nltk.word_tokenize(sentence.lower())\n",
      "        speech = nltk.pos_tag(token)\n",
      "        for pair in speech: \n",
      "        ############\n",
      "            if pair[1].startswith('NN') and pair[0]>2:\n",
      "                noun = noun + 1\n",
      "            if pair[1].startswith('JJ') and pair[0]>2:\n",
      "                adj = adj + 1\n",
      "            if pair[1].startswith('VB') and pair[0]>2:\n",
      "                vb = vb + 1\n",
      "            if pair[1].startswith('IN') and pair[0]>2:\n",
      "                pre = pre + 1\n",
      "            if pair[1].startswith('DT') and pair[0]>2:\n",
      "                dt = dt + 1\n",
      "        print (\"'NN' : \", noun,\"'JJ' : \", adj,\"'VB' : \", vb,\"'IN' : \", pre,\"'DT' : \", dt)        \n",
      "    \n",
      "    #for sentence in sentences:\n",
      "        #terms = nltk.word_tokenize(sentence.lower())\n",
      "        #tagged_terms=nltk.pos_tag(terms)\n",
      "        \n",
      "pos_counts(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NUMBER OF SENTENCES: 40\n",
        "(\"'NN' : \", 17, \"'JJ' : \", 1, \"'VB' : \", 3, \"'IN' : \", 5, \"'DT' : \", 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 2, \"'JJ' : \", 1, \"'VB' : \", 1, \"'IN' : \", 1, \"'DT' : \", 2)\n",
        "(\"'NN' : \", 8, \"'JJ' : \", 0, \"'VB' : \", 2, \"'IN' : \", 2, \"'DT' : \", 1)\n",
        "(\"'NN' : \", 10, \"'JJ' : \", 8, \"'VB' : \", 7, \"'IN' : \", 3, \"'DT' : \", 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 7, \"'JJ' : \", 5, \"'VB' : \", 5, \"'IN' : \", 5, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 8, \"'JJ' : \", 6, \"'VB' : \", 6, \"'IN' : \", 5, \"'DT' : \", 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 9, \"'JJ' : \", 1, \"'VB' : \", 4, \"'IN' : \", 4, \"'DT' : \", 4)\n",
        "(\"'NN' : \", 7, \"'JJ' : \", 2, \"'VB' : \", 1, \"'IN' : \", 3, \"'DT' : \", 1)\n",
        "(\"'NN' : \", 6, \"'JJ' : \", 3, \"'VB' : \", 5, \"'IN' : \", 2, \"'DT' : \", 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 3, \"'JJ' : \", 1, \"'VB' : \", 3, \"'IN' : \", 2, \"'DT' : \", 1)\n",
        "(\"'NN' : \", 10, \"'JJ' : \", 4, \"'VB' : \", 5, \"'IN' : \", 6, \"'DT' : \", 4)\n",
        "(\"'NN' : \", 13, \"'JJ' : \", 5, \"'VB' : \", 7, \"'IN' : \", 4, \"'DT' : \", 5)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 8, \"'JJ' : \", 8, \"'VB' : \", 5, \"'IN' : \", 5, \"'DT' : \", 6)\n",
        "(\"'NN' : \", 4, \"'JJ' : \", 3, \"'VB' : \", 2, \"'IN' : \", 1, \"'DT' : \", 0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 3, \"'JJ' : \", 1, \"'VB' : \", 2, \"'IN' : \", 1, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 6, \"'JJ' : \", 2, \"'VB' : \", 3, \"'IN' : \", 4, \"'DT' : \", 1)\n",
        "(\"'NN' : \", 10, \"'JJ' : \", 5, \"'VB' : \", 3, \"'IN' : \", 5, \"'DT' : \", 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 6, \"'JJ' : \", 0, \"'VB' : \", 4, \"'IN' : \", 3, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 4, \"'JJ' : \", 1, \"'VB' : \", 2, \"'IN' : \", 0, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 2, \"'JJ' : \", 2, \"'VB' : \", 2, \"'IN' : \", 1, \"'DT' : \", 0)\n",
        "(\"'NN' : \", 4, \"'JJ' : \", 1, \"'VB' : \", 1, \"'IN' : \", 1, \"'DT' : \", 0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 9, \"'JJ' : \", 3, \"'VB' : \", 2, \"'IN' : \", 2, \"'DT' : \", 0)\n",
        "(\"'NN' : \", 5, \"'JJ' : \", 1, \"'VB' : \", 4, \"'IN' : \", 2, \"'DT' : \", 2)\n",
        "(\"'NN' : \", 2, \"'JJ' : \", 0, \"'VB' : \", 5, \"'IN' : \", 1, \"'DT' : \", 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 1, \"'JJ' : \", 0, \"'VB' : \", 1, \"'IN' : \", 0, \"'DT' : \", 1)\n",
        "(\"'NN' : \", 4, \"'JJ' : \", 0, \"'VB' : \", 3, \"'IN' : \", 4, \"'DT' : \", 1)\n",
        "(\"'NN' : \", 9, \"'JJ' : \", 2, \"'VB' : \", 6, \"'IN' : \", 3, \"'DT' : \", 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 4, \"'JJ' : \", 1, \"'VB' : \", 2, \"'IN' : \", 1, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 14, \"'JJ' : \", 1, \"'VB' : \", 6, \"'IN' : \", 7, \"'DT' : \", 8)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 10, \"'JJ' : \", 3, \"'VB' : \", 4, \"'IN' : \", 3, \"'DT' : \", 4)\n",
        "(\"'NN' : \", 9, \"'JJ' : \", 1, \"'VB' : \", 2, \"'IN' : \", 2, \"'DT' : \", 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 5, \"'JJ' : \", 0, \"'VB' : \", 3, \"'IN' : \", 1, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 13, \"'JJ' : \", 4, \"'VB' : \", 5, \"'IN' : \", 8, \"'DT' : \", 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 13, \"'JJ' : \", 2, \"'VB' : \", 5, \"'IN' : \", 5, \"'DT' : \", 5)\n",
        "(\"'NN' : \", 16, \"'JJ' : \", 3, \"'VB' : \", 5, \"'IN' : \", 4, \"'DT' : \", 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 12, \"'JJ' : \", 0, \"'VB' : \", 4, \"'IN' : \", 4, \"'DT' : \", 3)\n",
        "(\"'NN' : \", 9, \"'JJ' : \", 2, \"'VB' : \", 3, \"'IN' : \", 5, \"'DT' : \", 6)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(\"'NN' : \", 10, \"'JJ' : \", 8, \"'VB' : \", 5, \"'IN' : \", 7, \"'DT' : \", 4)\n",
        "(\"'NN' : \", 1, \"'JJ' : \", 0, \"'VB' : \", 1, \"'IN' : \", 0, \"'DT' : \", 0)\n",
        "(\"'NN' : \", 4, \"'JJ' : \", 2, \"'VB' : \", 4, \"'IN' : \", 3, \"'DT' : \", 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Q 6\n",
      "def sort(file)\n",
      "    a=[]\n",
      "    b=[]\n",
      "    for l in open(file):\n",
      "        a.append(l)\n",
      "    for i in a:\n",
      "        b.append(i.split(','))\n",
      "     \n",
      "    for j in range(0,len(b)):\n",
      "        for k in range(j,len(b)):\n",
      "            e=b[j]\n",
      "            f=b[k]\n",
      "            e1=(e[1])\n",
      "            f1=(f[1])\n",
      "            if(e1 > f1):\n",
      "                t=e;\n",
      "                b[j]=f;\n",
      "                b[k]=t;\n",
      "            elif(e1 == f1):\n",
      "                t=e;\n",
      "                b[j]=f;\n",
      "                b[k]=t;\n",
      "    for i in b:\n",
      "        print(i)\n",
      "        \n",
      "sort_by_age('')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}